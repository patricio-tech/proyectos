{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled44.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODgS4bnvEjl4u2mOpTofoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricio-tech/proyectos/blob/master/RL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOcvh6ukvN6V"
      },
      "source": [
        "**V-function in Practice for Frozen-Lake Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL4LryGRvXpo"
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "#ENV_NAME = \"FrozenLake8x8-v0\"  \n",
        "GAMMA = 0.95"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qngxRFu86SPz"
      },
      "source": [
        "**The Agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-94gns06Zal"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV_NAME)\n",
        "        self.V = np.zeros(self.env.observation_space.n)\n",
        "\n",
        "    def calc_action_value(self, state, action):\n",
        "        action_value = sum([prob*(r + GAMMA * self.V[s_])\n",
        "                          for prob, s_, r, _ in self.env.P[state][action]]) \n",
        "        return action_value\n",
        "\n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(self.env.action_space.n):\n",
        "            action_value = self.calc_action_value(state, action)\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "\n",
        "    def value_iteration(self):\n",
        "        for state in range(self.env.observation_space.n):\n",
        "            state_values = [self.calc_action_value(state, action)\n",
        "                           for action in range(self.env.action_space.n)\n",
        "                           ]\n",
        "            self.V[state] = max(state_values)\n",
        "        return self.V"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGux6mpiExhW"
      },
      "source": [
        "Training **loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDgfK45GEzc6"
      },
      "source": [
        "TEST_EPISODES = 40\n",
        "REWARD_GOAL = 0.90\n",
        "\n",
        "def train(agent): \n",
        "  test_env = gym.make(ENV_NAME)\n",
        "  writer = SummaryWriter()\n",
        "\n",
        "  iter_no = 0\n",
        "  best_reward = 0.0\n",
        " \n",
        "  while best_reward < REWARD_GOAL:\n",
        "\n",
        "        #step 1\n",
        "        agent.value_iteration()\n",
        "\n",
        "        #step 2 check the improvements \n",
        "        iter_no += 1\n",
        "        reward_test = 0.0\n",
        "        for _ in range(TEST_EPISODES):\n",
        "            total_reward = 0.0\n",
        "            state = test_env.reset()\n",
        "            while True:\n",
        "                action = agent.select_action(state)\n",
        "                new_state, new_reward, is_done, _ = test_env.step(action)\n",
        "                total_reward += new_reward\n",
        "                if is_done: break\n",
        "                state = new_state\n",
        "            reward_test += total_reward\n",
        "        reward_test /= TEST_EPISODES\n",
        "\n",
        "        #step track with TensorBoard \n",
        "        writer.add_scalar(\"reward\", reward_test, iter_no)\n",
        "        if reward_test > best_reward:\n",
        "            print(\"Best reward updated %.2f at iteration %d \" % (reward_test ,iter_no) )\n",
        "            best_reward = reward_test\n",
        "\n",
        "  writer.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eViEYJoME_-d"
      },
      "source": [
        "**Training the Agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsXLhWWQFCUw",
        "outputId": "3e28e077-9ff8-4b11-c9c1-89cbaa792a81"
      },
      "source": [
        "agent = Agent()\n",
        "train(agent)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best reward updated 0.20 at iteration 3 \n",
            "Best reward updated 0.28 at iteration 4 \n",
            "Best reward updated 0.45 at iteration 5 \n",
            "Best reward updated 0.53 at iteration 7 \n",
            "Best reward updated 0.70 at iteration 13 \n",
            "Best reward updated 0.80 at iteration 17 \n",
            "Best reward updated 0.82 at iteration 34 \n",
            "Best reward updated 0.85 at iteration 55 \n",
            "Best reward updated 0.88 at iteration 95 \n",
            "Best reward updated 0.93 at iteration 550 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}